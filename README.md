# ğŸ©º Medical Chatbot - RAG-Based AI Assistant

A sophisticated medical information assistant powered by Retrieval-Augmented Generation (RAG) technology, combining local language models with medical document knowledge base for accurate, contextual responses.

![Medical Chatbot](https://img.shields.io/badge/Medical-Chatbot-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8%2B-brightgreen.svg)
![LangChain](https://img.shields.io/badge/LangChain-Latest-orange.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red.svg)
![Ollama](https://img.shields.io/badge/Ollama-LLM-purple.svg)

## ğŸŒŸ Features

- **ğŸ¤– RAG-Powered Responses**: Combines document retrieval with AI generation for accurate medical information
- **ğŸ  Fully Local Operation**: No external API dependencies, complete privacy and control
- **âš¡ Fast Performance**: Local vector database and LLM for quick responses
- **ğŸ“š Medical Knowledge Base**: Processes PDF medical documents for contextual answers
- **ğŸ” Semantic Search**: Advanced embeddings for relevant document retrieval
- **ğŸ’¬ Interactive UI**: Beautiful Streamlit web interface
- **ğŸ”§ Easy Management**: Built-in database management and system status monitoring
- **âš ï¸ Safety First**: Clear medical disclaimers and responsible AI practices

## ğŸ› ï¸ Tech Stack

### **Backend**
- **ğŸ Python 3.8+**: Core programming language
- **ğŸ¦œ LangChain**: Framework for building LLM applications
- **ğŸ¦™ Ollama**: Local LLM inference server
- **ğŸ“Š ChromaDB**: Local vector database for embeddings storage
- **ğŸ¤— HuggingFace**: Embedding models (`BAAI/bge-small-en-v1.5`)
- **ğŸ“„ PyPDF**: PDF document processing

### **Frontend Options**
- **ğŸ¨ Streamlit**: Interactive web application (Recommended)
- **ğŸš€ FastAPI**: REST API with built-in web interface
- **ğŸ’» HTML/CSS/JS**: Custom web interface

### **AI Models**
- **Language Models**: 
  - **Local**: Llama 3.2 (3B/8B) via Ollama - Privacy-focused inference
  - **Cloud**: Llama 3.1 (8B) via Groq - Free open-source models
- **Embeddings**: BAAI/bge-small-en-v1.5 - Semantic understanding
- **Vector Store**: ChromaDB - Local similarity search

### **Deployment**
- **ğŸ³ Docker**: Containerized deployment
- **â˜ï¸ Cloud Platforms**: Streamlit Cloud, Railway, Heroku
- **ğŸ” API Services**: Groq (Free), Ollama (Local)

## ğŸ“‹ Prerequisites

- **Python 3.8 or higher**
- **Ollama installed**: [Installation Guide](https://ollama.ai/)
- **4GB+ RAM**: For running local LLM models
- **PDF Documents**: Medical documents for the knowledge base

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/Godwill98/Compete-Medical-chatbot-Llms.git
cd Compete-Medical-chatbot-Llms
```

### 2. Set Up Python Environment
```bash
# Create virtual environment
python -m venv medibot
source medibot/bin/activate  # On Windows: medibot\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Install and Set Up Ollama
```bash
# Install Ollama (visit https://ollama.ai for OS-specific instructions)

# Pull the required model
ollama pull llama3.2:3b

# Start Ollama server
ollama serve
```

### 4. Prepare Your Medical Documents
```bash
# Create data directory and add your PDF files
mkdir data
# Copy your medical PDF files to the data/ directory
```

### 5. Create Local Vector Database
```bash
python create_local_db.py
```

### 6. Run the Application
```bash
# Option 1: Streamlit Web App (Recommended)
streamlit run streamlit_app.py

# Option 2: FastAPI Server
python fastapi_app.py

# Option 3: Cloud-Compatible App (for deployment)
streamlit run streamlit_app_cloud.py
```

## ğŸŒ Cloud Deployment

For cloud deployment, use `streamlit_app_cloud.py` which supports **Groq API** for free open-source models:

### **Environment Variables for Cloud**
```bash
# Required for cloud deployment
GROQ_API_KEY=your_groq_api_key_here

# Optional deployment flags (auto-detected)
STREAMLIT_SHARING=true
RAILWAY_ENVIRONMENT=true
```

### **Get Groq API Key (Free)**
1. Visit [Groq Console](https://console.groq.com/)
2. Sign up for a free account
3. Generate your API key
4. Set it in your deployment platform's environment variables

### **Supported Cloud Platforms**
- **Streamlit Cloud**: Direct deployment from GitHub
- **Railway**: One-click deployment
- **Heroku**: Using Docker or buildpacks
- **Render**: Static site deployment

## ğŸ“ Project Structure

```
Compete-Medical-chatbot-Llms/
â”œâ”€â”€ ğŸ“„ README.md                 # Project documentation
â”œâ”€â”€ ğŸ“‹ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ”§ setup.py                  # Package setup
â”œâ”€â”€ ğŸŒ .env                      # Environment variables (optional)
â”œâ”€â”€ ğŸ“š data/                     # PDF medical documents
â”‚   â””â”€â”€ Medical_book.pdf
â”œâ”€â”€ ğŸ—ƒï¸ chroma_db/               # Local vector database
â”œâ”€â”€ ğŸ”¬ research/                 # Jupyter notebooks for experimentation
â”‚   â””â”€â”€ trials.ipynb
â”œâ”€â”€ ğŸ“¦ src/                      # Source code modules
â”‚   â”œâ”€â”€ helper.py
â”‚   â””â”€â”€ prompt.py
â”œâ”€â”€ ğŸ¨ streamlit_app.py          # Streamlit web application
â”œâ”€â”€ ğŸš€ fastapi_app.py            # FastAPI server application
â”œâ”€â”€ ğŸ”¨ create_local_db.py        # Vector database creation script
â”œâ”€â”€ ğŸ“± app.py                    # Alternative Flask application
â””â”€â”€ ğŸ¤– medibot/                  # Virtual environment
```

## ğŸ® Usage

### **Streamlit Interface**
1. Open your browser to `http://localhost:8501`
2. Use the sidebar to:
   - Check system status
   - Test connections
   - Manage database
   - Clear chat history
3. Type medical questions in the chat input
4. Get AI-powered responses based on your documents

### **FastAPI Interface**
1. Open your browser to `http://localhost:8000`
2. Use the web interface for chat
3. Access API documentation at `http://localhost:8000/docs`

### **Sample Questions**
- "What is diabetes and its symptoms?"
- "How is hypertension treated?"
- "What are the side effects of aspirin?"
- "Explain the diagnosis process for pneumonia"

## âš™ï¸ Configuration

### **Model Selection**
Edit the model in your app files:
```python
# For different Ollama models
model="llama3.1:8b"    # Larger model, better quality
model="llama3.2:3b"    # Smaller model, faster inference
model="mistral:7b"     # Alternative model
```

### **Embedding Models**
```python
# Different embedding models
model_name = "BAAI/bge-small-en-v1.5"    # Default (384 dimensions)
model_name = "all-MiniLM-L6-v2"          # Alternative (384 dimensions)
```

### **Vector Database Settings**
```python
# Retrieval settings
search_kwargs={"k": 3}        # Number of documents to retrieve
chunk_size=500               # Document chunk size
chunk_overlap=20             # Overlap between chunks
```

## ğŸ”§ Advanced Features

### **Database Management**
- **Rebuild Database**: Add new PDFs and recreate the vector store
- **Status Monitoring**: Check system health and connections
- **Cache Management**: Clear cached resources for fresh starts

### **API Endpoints (FastAPI)**
- `POST /chat`: Send messages to the chatbot
- `GET /health`: Check system health
- `GET /`: Web interface

## ğŸš¨ Important Disclaimers

âš ï¸ **This chatbot is for informational purposes only**
- Always consult healthcare professionals for medical advice
- Not intended for emergency medical situations
- Responses based on training data, may not reflect latest research
- Should not replace professional medical diagnosis or treatment

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LangChain Community** for the excellent framework
- **Ollama Team** for local LLM inference
- **HuggingFace** for embedding models
- **ChromaDB** for vector database solution
- **Streamlit** for the amazing web app framework

## ğŸ“ Support

- ğŸ› **Issues**: [GitHub Issues](https://github.com/Godwill98/Compete-Medical-chatbot-Llms/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/Godwill98/Compete-Medical-chatbot-Llms/discussions)
- ğŸ“§ **Contact**: [Your Email]

## ğŸ”® Future Enhancements

- [ ] Multi-language support
- [ ] Voice interaction capabilities
- [ ] Integration with medical databases
- [ ] Advanced analytics and reporting
- [ ] Mobile application
- [ ] Doctor consultation scheduling
- [ ] Symptom checker integration

---

**Built with â¤ï¸ by [Godwill Kiplagat](https://github.com/Godwill98)**

*Empowering healthcare through AI technology*